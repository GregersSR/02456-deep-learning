{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scaled_dataloader import load_train, load_val\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_trajectory import plot_paths\n",
    "#from metrics import rmse, mse, mae\n",
    "import joblib\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer Model\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "                        * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "\n",
    "        pe = pe.unsqueeze(0)     # â†’ [1, max_len, d_model]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class TrajectoryTransformer30to10(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 3,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.future_steps = 10\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,              # <-- CHANGED\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.out = nn.Linear(d_model, output_dim * self.future_steps)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        src: [batch, 30, input_dim]\n",
    "        return: [batch, 10, output_dim]\n",
    "        \"\"\"\n",
    "        x = self.input_proj(src)       # [B, 30, d_model]\n",
    "        x = self.pos_enc(x)            # [B, 30, d_model]\n",
    "        x = self.encoder(x)            # [B, 30, d_model]\n",
    "\n",
    "        last_state = x[:, -1, :]       # [B, d_model]\n",
    "\n",
    "        out = self.out(last_state)     # [B, 10 * output_dim]\n",
    "\n",
    "        return out.reshape(-1, 10, self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, scaler = load_train()\n",
    "val_ds = load_val(scaler)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds, batch_size=64, shuffle=False\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=64, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations:\n",
    "\n",
    "#Test different Models\n",
    "'''{   \n",
    "        \"name\": \"Mini_Model\",\n",
    "        \"model_kwargs\": {\n",
    "            \"input_dim\": 2,\n",
    "            \"output_dim\": 2,\n",
    "            \"d_model\": 128,\n",
    "            \"nhead\": 4,\n",
    "            \"num_layers\": 3,\n",
    "            \"dim_feedforward\": 10,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "        \"train_kwargs\": {\n",
    "            \"num_epochs\": 10,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"weight_decay\": 1e-4,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"A_small\",\n",
    "        \"model_kwargs\": {\n",
    "            \"input_dim\": 2,\n",
    "            \"output_dim\": 2,\n",
    "            \"d_model\": 128,\n",
    "            \"nhead\": 4,\n",
    "            \"num_layers\": 3,\n",
    "            \"dim_feedforward\": 512,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "        \"train_kwargs\": {\n",
    "            \"num_epochs\": 40,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"weight_decay\": 1e-4,\n",
    "        },\n",
    "    },\n",
    "       {\n",
    "        \"name\": \"Mini_Test\",\n",
    "        \"model_kwargs\": {\n",
    "            \"input_dim\": 2,\n",
    "            \"output_dim\": 2,\n",
    "            \"d_model\": 128,\n",
    "            \"nhead\": 8,\n",
    "            \"num_layers\": 2,\n",
    "            \"dim_feedforward\": 100,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "        \"train_kwargs\": {\n",
    "            \"num_epochs\": 5,\n",
    "            \"learning_rate\": 5e-4,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"patience\": 5,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"C_deeper_2\",\n",
    "        \"model_kwargs\": {\n",
    "            \"input_dim\": 2,\n",
    "            \"output_dim\": 2,\n",
    "            \"d_model\": 256,\n",
    "            \"nhead\": 8,\n",
    "            \"num_layers\": 4,\n",
    "            \"dim_feedforward\": 1024,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "        \"train_kwargs\": {\n",
    "            \"num_epochs\": 90,\n",
    "            \"learning_rate\": 5e-4,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"patience\": 5,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"C_deeper\",\n",
    "        \"model_kwargs\": {\n",
    "            \"input_dim\": 2,\n",
    "            \"output_dim\": 2,\n",
    "            \"d_model\": 256,\n",
    "            \"nhead\": 8,\n",
    "            \"num_layers\": 4,\n",
    "            \"dim_feedforward\": 1024,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "        \"train_kwargs\": {\n",
    "            \"num_epochs\": 50,\n",
    "            \"learning_rate\": 5e-4,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"patience\": 5,\n",
    "        },\n",
    "    },\n",
    "'''\n",
    "\n",
    "#Right now it only trains for B_medium\n",
    "configs = [\n",
    "     {\n",
    "        \"name\": \"B_medium\",\n",
    "        \"model_kwargs\": {\n",
    "            \"input_dim\": 2,\n",
    "            \"output_dim\": 2,\n",
    "            \"d_model\": 256,\n",
    "            \"nhead\": 8,\n",
    "            \"num_layers\": 3,\n",
    "            \"dim_feedforward\": 1024,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "        \"train_kwargs\": {\n",
    "            \"num_epochs\": 40,\n",
    "            \"learning_rate\": 5e-4,\n",
    "            \"weight_decay\": 1e-4,\n",
    "        },\n",
    "     },\n",
    "  \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop I used for the models\n",
    "\n",
    "\n",
    "\n",
    "def train_one_config(config, train_loader, val_loader, device):\n",
    "    name = config[\"name\"]\n",
    "    model_kwargs = config[\"model_kwargs\"]\n",
    "    train_kwargs = config[\"train_kwargs\"]\n",
    "\n",
    "    num_epochs    = train_kwargs[\"num_epochs\"]\n",
    "    learning_rate = train_kwargs[\"learning_rate\"]\n",
    "    weight_decay  = train_kwargs[\"weight_decay\"]\n",
    "    patience      = train_kwargs.get(\"patience\", None)  # z.B. 5 oder None\n",
    "\n",
    "    print(f\"\\n=== Training config: {name} ===\")\n",
    "    print(\"Model args:\", model_kwargs)\n",
    "    print(\"Train args:\", train_kwargs)\n",
    "\n",
    "    model = TrajectoryTransformer30to10(**model_kwargs).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_mse\": [],\n",
    "        \"train_rmse\": [],\n",
    "        \"train_mae\": [],\n",
    "        \"val_mse\": [],\n",
    "        \"val_rmse\": [],\n",
    "        \"val_mae\": [],\n",
    "    }\n",
    "\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    best_state = None\n",
    "    best_epoch = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        train_mse = 0.0\n",
    "        train_mae = 0.0\n",
    "        n_train   = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            X, Y = batch if len(batch) == 2 else batch[:2]\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X)\n",
    "            loss = criterion(preds, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            mse_batch = loss.item()\n",
    "            mae_batch = torch.mean(torch.abs(preds - Y)).item()\n",
    "            bs = X.size(0)\n",
    "\n",
    "            train_mse += mse_batch * bs\n",
    "            train_mae += mae_batch * bs\n",
    "            n_train   += bs\n",
    "\n",
    "        train_mse /= n_train\n",
    "        train_rmse = math.sqrt(train_mse)\n",
    "        train_mae /= n_train\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        val_mse = 0.0\n",
    "        val_mae = 0.0\n",
    "        n_val   = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                X_val, Y_val = batch if len(batch) == 2 else batch[:2]\n",
    "                X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
    "\n",
    "                preds_val = model(X_val)\n",
    "                loss_val  = criterion(preds_val, Y_val)\n",
    "\n",
    "                mse_batch = loss_val.item()\n",
    "                mae_batch = torch.mean(torch.abs(preds_val - Y_val)).item()\n",
    "                bs = X_val.size(0)\n",
    "\n",
    "                val_mse += mse_batch * bs\n",
    "                val_mae += mae_batch * bs\n",
    "                n_val   += bs\n",
    "\n",
    "        val_mse /= n_val\n",
    "        val_rmse = math.sqrt(val_mse)\n",
    "        val_mae /= n_val\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_mse\"].append(train_mse)\n",
    "        history[\"train_rmse\"].append(train_rmse)\n",
    "        history[\"train_mae\"].append(train_mae)\n",
    "        history[\"val_mse\"].append(val_mse)\n",
    "        history[\"val_rmse\"].append(val_rmse)\n",
    "        history[\"val_mae\"].append(val_mae)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Train RMSE={train_rmse:.4f}, Val RMSE={val_rmse:.4f}\"\n",
    "        )\n",
    "\n",
    "        # ---- Early Stopping / remember best model ----\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_epoch = epoch\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if patience is not None and epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    # reopen best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"Loaded best model from epoch {best_epoch} with Val RMSE={best_val_rmse:.4f}\")\n",
    "\n",
    "    final_val_rmse = best_val_rmse\n",
    "    final_val_mse  = None  \n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"config\": config,\n",
    "        \"history\": history,\n",
    "        \"final_val_rmse\": final_val_rmse,\n",
    "        \"final_val_mse\": final_val_mse,\n",
    "        \"model\": model,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part should be flexible -> the things you want to see, here I'm currently not saving the checkpoint\n",
    "results = []\n",
    "best_result = None\n",
    "\n",
    "for cfg in configs:\n",
    "    result = train_one_config(cfg, train_loader, val_loader, device)\n",
    "    results.append(result)\n",
    "\n",
    "    if best_result is None or result[\"final_val_rmse\"] < best_result[\"final_val_rmse\"]:\n",
    "        best_result = result\n",
    "\n",
    "print(\"\\n======================\")\n",
    "print(\"Best config:\", best_result[\"name\"])\n",
    "print(\"Best final Val RMSE:\", best_result[\"final_val_rmse\"])\n",
    "print(\"Best model args:\", best_result[\"config\"][\"model_kwargs\"])\n",
    "print(\"Best train args:\", best_result[\"config\"][\"train_kwargs\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
