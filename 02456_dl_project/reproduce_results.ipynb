{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dfae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_trajectory import plot_paths\n",
    "import training\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import load_val\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from model_autoregressive import Seq2SeqLSTM\n",
    "from transformer_model import TrajectoryTransformer30to10\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from paths import RESULTS_FILTERED_DIR, RESULTS_UNFILTERED_DIR\n",
    "from model_selection import rank_models, plot_model_losses, haversine_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd24e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = training.determine_device()\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f42843",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"val_mse\", \"val_rmse\", \"val_mae\"]\n",
    "best_model_name, best_score, best_model_data = rank_models(RESULTS_FILTERED_DIR, metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses for the best model based on MSE\n",
    "\n",
    "plot_model_losses(best_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "batch_size = 512\n",
    "scaler_filtered = joblib.load(\"scaler_filtered.save\")\n",
    "val_ds_filtered = load_val(filter_stationary=True, scaler=scaler_filtered)\n",
    "scaler_unfiltered = joblib.load(\"scaler_unfiltered.save\")\n",
    "val_ds_unfiltered = load_val(filter_stationary=False, scaler=scaler_unfiltered)\n",
    "val_loader_filtered = DataLoader(val_ds_filtered, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "val_loader_unfiltered = DataLoader(val_ds_unfiltered, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b492cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"deeper_autoreg_lstm_2_best.pt\" \n",
    "full_best_model_path = os.path.join(RESULTS_FILTERED_DIR, best_model_path)\n",
    "\n",
    "model = Seq2SeqLSTM(**best_model_data[\"config\"][\"model_kwargs\"]).to(device)\n",
    "model.load_state_dict(torch.load(full_best_model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_path = \"deeper_transformer_best.pt\" \n",
    "# full_best_model_path = os.path.join(RESULTS_FILTERED_DIR, best_model_path)\n",
    "\n",
    "# model = Seq2SeqLSTM(**best_model_data[\"config\"][\"model_kwargs\"]).to(device)\n",
    "# model.load_state_dict(torch.load(full_best_model_path, map_location=device))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute per-sample MSE on validation set (notice here we do per-sample MSE, so the total loss is 20 times smaller)\n",
    "all_mse = []\n",
    "all_samples = []\n",
    "\n",
    "for x, y in tqdm(val_loader_filtered, desc=\"Computing per-sample MSE\"):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "    \n",
    "    mse_per_sample = torch.mean((y_pred - y)**2, dim=[1,2])\n",
    "    all_mse.append(mse_per_sample.cpu().numpy())\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        all_samples.append({\n",
    "            \"x\": x[i].cpu().numpy(),\n",
    "            \"y\": y[i].cpu().numpy(),\n",
    "            \"y_pred\": y_pred[i].cpu().numpy()\n",
    "        })\n",
    "\n",
    "all_mse = np.concatenate(all_mse)\n",
    "sorted_indices = np.argsort(all_mse)\n",
    "\n",
    "n = len(all_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdce197",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [10, 30, 50, 80, 90, 95, 99, 99.99, 100]\n",
    "print(\"\\nMSE percentile summary:\\n\")\n",
    "for p in percentiles:\n",
    "    threshold = np.percentile(all_mse, p)\n",
    "    print(f\"{p:>3}% of samples have MSE ≤ {threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {\n",
    "    \"Best\": sorted_indices[:3],\n",
    "    \"Q1\": sorted_indices[n//4:n//4+3],\n",
    "    \"Median\": sorted_indices[n//2:n//2+3],\n",
    "    \"Q3\": sorted_indices[3*n//4:3*n//4+3],\n",
    "    \"Worst\": sorted_indices[-3:]\n",
    "}\n",
    "\n",
    "def plot_sample(sample, title):\n",
    "    plot_paths(sample[\"x\"], sample[\"y\"], sample[\"y_pred\"], title, scaler=scaler_filtered)\n",
    "\n",
    "for group_name, indices in groups.items():\n",
    "    print(f\"\\nPlotting 3 samples from {group_name} group:\")\n",
    "    for idx in indices:\n",
    "        plot_sample(all_samples[idx], f\"{group_name} Sample (MSE={all_mse[idx]:.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86719f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Haversine Distance Evaluation by Groups ===\")\n",
    "\n",
    "for group_name, indices in groups.items():\n",
    "    group_means = []\n",
    "\n",
    "    print(f\"\\n### {group_name} group ###\")\n",
    "\n",
    "    for idx in indices:\n",
    "        sample = all_samples[idx]\n",
    "\n",
    "        # inverse scale\n",
    "        y_true_scaled = sample[\"y\"]\n",
    "        y_pred_scaled = sample[\"y_pred\"]\n",
    "        y_true_unscaled = scaler_filtered.inverse_transform(y_true_scaled)\n",
    "        y_pred_unscaled = scaler_filtered.inverse_transform(y_pred_scaled)\n",
    "\n",
    "        # Compute Haversine\n",
    "        dists_km, mean_hav_km = haversine_np(y_true_unscaled, y_pred_unscaled)\n",
    "\n",
    "        # Save group mean\n",
    "        group_means.append(mean_hav_km)\n",
    "\n",
    "        # --- Pretty step-wise print for groups ---\n",
    "        print(f\"\\nSample {idx} (MSE={all_mse[idx]:.6f}) – Haversine per step:\")\n",
    "        for step, d in enumerate(dists_km, start=1):\n",
    "            print(f\"  Step {step:02d} → {d:10.6f} km\")\n",
    "\n",
    "        print(f\"  → Mean Haversine for this sample: {mean_hav_km:.6f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2192347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
